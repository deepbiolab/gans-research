# Configuration for WGAN on CelebA
experiment:
  name: wgan_celeba
  seed: 42
  output_dir: ./outputs/wgan
  log_interval: 200
  sampling_num: 16
  save_interval: 1000
  num_epochs: 10
  num_workers: 8
  device: cuda

data:
  name: celeba
  batch_size: 64
  image_size: 64
  channels: 3
  crop_size: 108

model:
  name: wgan
  latent_dim: 100
  ngf: 64
  ndf: 64
  use_batch_norm: true

training:
  optimizer: rmsprop  # WGAN originally used RMSProp
  lr_g: 0.00005
  lr_d: 0.00005
  n_critic: 5  # Update critic 5 times for each generator update
  use_weight_clipping: true
  clip_value: [-0.01, 0.01]

evaluation:
  use_fid: false
  batch_size: 64
  num_samples: 2000
  eval_epoch_interval: 1

inference:
  checkpoint_path: ./outputs/wgan/checkpoints/final_model.pth
  num_samples: 16

logging:
  use_tensorboard: false
  use_wandb: true
  wandb_project: gans-research

# Tricks for stable training
ema:
  enable: false
  beta: 0.995
  update_after_step: 100
  update_every: 1

augment:
  enable: false
  policy: "color,translation"