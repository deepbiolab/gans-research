# Configuration for WGAN on CelebA
experiment:
  name: wgan_celeba
  seed: 42
  output_dir: ./outputs/wgan
  log_interval: 100
  sampling_num: 16
  save_interval: 1000
  num_epochs: 100
  num_workers: 4
  device: cuda

data:
  name: celeba
  batch_size: 64
  image_size: 64
  channels: 3
  crop_size: 108

model:
  name: wgan
  latent_dim: 100
  ngf: 64
  ndf: 64
  use_batch_norm: false  # WGAN often uses layer norm instead
  use_layer_norm: true

training:
  optimizer: rmsprop  # WGAN originally used RMSProp
  lr_g: 0.00005
  lr_d: 0.00005
  n_critic: 5  # Update critic 5 times for each generator update
  weight_clip: 0.01  # Weight clipping parameter

evaluation:
  use_fid: false
  batch_size: 64
  num_samples: 2000
  eval_epoch_interval: 1

inference:
  checkpoint_path: ./outputs/vanilla_gan/checkpoints/final_model.pth
  num_samples: 16

logging:
  use_tensorboard: true
  use_wandb: false
  wandb_project: gans-research