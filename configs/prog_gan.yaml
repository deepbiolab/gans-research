# Configuration for Progressive GAN on MINIST
experiment:
  name: prog_gan_minist
  seed: 42
  output_dir: ./outputs/prog_gan
  log_interval: 200
  sampling_num: 16
  save_interval: 1000
  device: cuda
  
data:
  name: minist
  channels: 3
  # Progressive training uses different batch sizes for different resolutions
  batch_sizes:
    4: 128
    8: 128
    16: 128
    32: 64
    64: 32
    128: 16
    256: 8
    512: 4
    1024: 2

model:
  name: prog_gan
  latent_dim: 512
  feature_maps: 16  # Base feature maps, doubles with each resolution decrease
  use_minibatch_stddev: true
  use_equalized_lr: true
  use_pixelnorm: true

training:
  optimizer: adam
  lr_g: 0.001
  lr_d: 0.001
  beta1: 0.0
  beta2: 0.99
  epsilon: 1e-8
  lambda_drift: 0.001  # Epsilon drift penalty
  
  # Progressive growing parameters
  starting_resolution: 4
  target_resolution: 1024
  phase_iterations: 600000  # Number of images shown before increasing resolution
  transition_iterations: 600000  # Number of images for fade-in transition

evaluation:
  use_fid: true
  batch_size: 64
  num_samples: 100
  eval_epoch_interval: 1

inference:
  checkpoint_path: ./outputs/prog_gan/checkpoints/final_model.pth
  num_samples: 16

logging:
  use_tensorboard: true
  use_wandb: false
  wandb_project: gans-research

# Tricks for stable training
ema:
  enable: true
  beta: 0.995
  update_after_step: 100
  update_every: 1