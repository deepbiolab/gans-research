# Configuration for Vanilla GAN on MNIST
experiment:
  name: vanilla_gan_mnist
  seed: 42
  output_dir: ./outputs/vanilla_gan
  log_interval: 100
  sample_interval: 500
  save_interval: 1000
  num_epochs: 3
  num_workers: 4
  device: cuda

data:
  name: mnist
  batch_size: 64
  image_size: 28
  channels: 1

model:
  name: vanilla_gan
  latent_dim: 100
  hidden_dim: 256
  use_batch_norm: false  # Original GAN didn't use batch normalization

training:
  optimizer: adam
  lr_g: 0.0002
  lr_d: 0.0002
  beta1: 0.5
  beta2: 0.999
  use_scheduler: false
  scheduler_gamma: 0.99
  n_critic: 1  # Standard GAN updates G and D equally

logging:
  use_tensorboard: true
  use_wandb: false
  wandb_project: gans-research