# Configuration for Vanilla GAN on MNIST
experiment:
  name: vanilla_gan_mnist
  seed: 42
  output_dir: ./outputs/vanilla_gan
  log_interval: 100
  sampling_num: 16
  save_interval: 1000
  num_epochs: 3
  num_workers: 4
  device: cuda

data:
  name: mnist
  batch_size: 64
  image_size: 28
  channels: 1

model:
  name: vanilla_gan
  latent_dim: 100
  hidden_dim: 256
  use_batch_norm: false  # Original GAN didn't use batch normalization

training:
  optimizer: adam
  lr_g: 0.0002
  lr_d: 0.0002
  beta1: 0.5
  beta2: 0.999
  use_scheduler: false
  scheduler_gamma: 0.99
  n_critic: 1  # Standard GAN updates G and D equally

evaluation:
  use_fid: false
  fid_batch_size: 64
  fid_num_samples: 2000
  eval_epoch_interval: 1

inference:
  checkpoint_path: ./outputs/vanilla_gan/checkpoints/final_model.pth
  num_samples: 16

logging:
  use_tensorboard: false
  use_wandb: true
  wandb_project: gans-research