# Configuration for WGAN-GP on CelebA
experiment:
  name: wgan_gp_celeba
  seed: 42
  output_dir: ./outputs/wgan_gp
  log_interval: 200
  sampling_num: 16
  save_interval: 1000
  num_epochs: 1
  num_workers: 8
  device: cuda

data:
  name: celeba
  batch_size: 64
  image_size: 64
  channels: 3
  crop_size: 108

model:
  name: wgan_gp
  latent_dim: 100
  ngf: 64
  ndf: 64
  use_batch_norm: true

training:
  optimizer: adam  # WGAN-GP can use Adam with gradient penalty
  lr_g: 0.0001
  lr_d: 0.0001
  beta1: 0.0  # Different beta values for WGAN-GP
  beta2: 0.9
  n_critic: 5  # Update critic 5 times for each generator update
  lambda_gp: 10.0  # Gradient penalty coefficient

evaluation:
  use_fid: true
  batch_size: 64
  num_samples: 2000
  eval_epoch_interval: 1

inference:
  checkpoint_path: ./outputs/wgan_gp/checkpoints/final_model.pth
  num_samples: 16

logging:
  use_tensorboard: false
  use_wandb: true
  wandb_project: gans-research
